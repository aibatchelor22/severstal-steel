{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Severstal Steel Challenge\n",
    "\n",
    "This notebook is a submission for a Kaggle challenge hosted by Severstal Steel.  \n",
    "\n",
    "https://www.kaggle.com/c/severstal-steel-defect-detection\n",
    "\n",
    "The purpose is to build a segmentation model to label four types of manufacturing defects in images of steel.  I used the segmentation_models library to create UNet models with a pre determined backbone initialized with ImageNet weights.  For example, one can choose a ResNet34 backbone. \n",
    "\n",
    "https://github.com/qubvel/segmentation_models\n",
    "\n",
    "My best model used an ensemble of ResNet34, Inceptionv3, and DenseNet121. Performance is measured with a Dice Coefficient.  My highest score was 0.84.  The winning submission was 0.91. \n",
    "\n",
    "Please read my blog if you would like to learn more about my ideas and motivation for this challenge.\n",
    "\n",
    "https://www.funwithdatascience.com/2020/04/06/model-for-detecting-steel-defects-in-images/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Ashley\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Ashley\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Ashley\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Ashley\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Ashley\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Ashley\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Ashley\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Ashley\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Ashley\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Ashley\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Ashley\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Ashley\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `keras` framework.\n"
     ]
    }
   ],
   "source": [
    "#RLE functions from https://www.kaggle.com/paulorzp/rle-functions-run-lenght-encode-decode/code\n",
    "\n",
    "from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "from numpy import fliplr, flipud\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, BatchNormalization\n",
    "from keras.layers.core import Dropout, Lambda\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import segmentation_models\n",
    "\n",
    "\n",
    "def mask2rle(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels= img.T.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    " \n",
    "def rle2mask(mask_rle, shape=(1600,256)):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (width,height) of array to return \n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Operating Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT = 256\n",
    "IMG_WIDTH = 1600\n",
    "IMG_CHANNELS = 3\n",
    "DEFECT_CLASSES = 4\n",
    "SCALE_FACTOR = 2\n",
    "SAMPLE_SIZE = 950\n",
    "\n",
    "train = pd.read_csv(r'C:\\Users\\Ashley\\SeverstalSteel\\train.csv')\n",
    "IMAGE_PATH = r\"C:\\Users\\Ashley\\SeverstalSteel\\train_images\\\\\"\n",
    "TEST_IMAGE_PATH = r\"C:\\Users\\Ashley\\SeverstalSteel\\test_images\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Y Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import images\n",
    "from skimage import color\n",
    "from skimage import io\n",
    "\n",
    "#import masks\n",
    "class_indices = [[],[],[],[]]\n",
    "multiclass_image_list = []\n",
    "\n",
    "for ind in range(train.shape[0]):\n",
    "    #Check if multiple masks exist, if so, add to a separate list of images\n",
    "    if train['EncodedPixels'][ind] != '':\n",
    "        current_file_name = train['ImageId'][ind]\n",
    "        if train[train['ImageId'] == current_file_name].shape[0] == 1:\n",
    "            class_indices[train['ClassId'][ind]-1].append(ind)\n",
    "        elif current_file_name not in multiclass_image_list:\n",
    "            multiclass_image_list.append(current_file_name)\n",
    "            \n",
    "            \n",
    "\n",
    "    \n",
    "    \n",
    "Y_train = np.zeros((SAMPLE_SIZE*4, int(IMG_HEIGHT/SCALE_FACTOR), int(IMG_WIDTH/SCALE_FACTOR), int(DEFECT_CLASSES)), dtype=np.bool)\n",
    "\n",
    "\n",
    "#get a random sample from each class\n",
    "sample_set = []\n",
    "for defect_type in range(4):\n",
    "    partial_sample_set = np.random.permutation(int(SAMPLE_SIZE/5))\n",
    "    for sample_index in partial_sample_set:\n",
    "        sample_set.append(class_indices[defect_type][sample_index])\n",
    "\n",
    "#Populate training data for the single class images\n",
    "n=0        \n",
    "for ind in range(len(sample_set)):\n",
    "\n",
    "    img = rle2mask(train['EncodedPixels'][sample_set[ind]])\n",
    "    img = resize(img, (int(IMG_HEIGHT/SCALE_FACTOR), int(IMG_WIDTH/SCALE_FACTOR)), mode='constant', preserve_range=True)\n",
    "    Y_train[n,:,:,(train['ClassId'][sample_set[ind]]-1)] = img\n",
    "    Y_train[n+1,:,:,(train['ClassId'][sample_set[ind]]-1)] = fliplr(img)\n",
    "    Y_train[n+2,:,:,(train['ClassId'][sample_set[ind]]-1)] = flipud(img)\n",
    "    Y_train[n+3,:,:,(train['ClassId'][sample_set[ind]]-1)] = fliplr(flipud(img))\n",
    "    n+=4\n",
    "    \n",
    "#Handle multiple mask instances\n",
    "multiclass_samples = []\n",
    "partial_sample_set_indices = np.random.permutation(int(SAMPLE_SIZE/5))\n",
    "for ind in partial_sample_set_indices:\n",
    "    multiclass_samples.append(multiclass_image_list[ind])\n",
    "for current_file_name in multiclass_samples:\n",
    "    #make temporary dataframe corresponding to this image\n",
    "    df = train[train['ImageId'] == current_file_name]\n",
    "    df = df.reset_index(drop=True)\n",
    "    for ind in range(df.shape[0]):\n",
    "        #add each mask to an entry\n",
    "        img = rle2mask(df['EncodedPixels'][ind])\n",
    "        img = resize(img, (int(IMG_HEIGHT/SCALE_FACTOR), int(IMG_WIDTH/SCALE_FACTOR)), mode='constant', preserve_range=True)\n",
    "        Y_train[n,:,:,(df['ClassId'][ind]-1)] = img\n",
    "        Y_train[n+1,:,:,(df['ClassId'][ind]-1)] = fliplr(img)\n",
    "        Y_train[n+2,:,:,(df['ClassId'][ind]-1)] = flipud(img)\n",
    "        Y_train[n+3,:,:,(df['ClassId'][ind]-1)] = fliplr(flipud(img))\n",
    "    n += 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the X Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.zeros((SAMPLE_SIZE*4, int(IMG_HEIGHT/SCALE_FACTOR), int(IMG_WIDTH/SCALE_FACTOR), IMG_CHANNELS), dtype=np.uint8)\n",
    "#Populate X_train for single class images\n",
    "n = 0\n",
    "for ind in range(len(sample_set)):\n",
    "    #img = color.rgb2gray(io.imread(IMAGE_PATH + train['ImageId'][ind]))\n",
    "    img = imread(IMAGE_PATH + train['ImageId'][sample_set[ind]])\n",
    "    img = resize(img, (int(IMG_HEIGHT/SCALE_FACTOR), int(IMG_WIDTH/SCALE_FACTOR), IMG_CHANNELS), mode='constant', preserve_range=True)\n",
    "    X_train[n,:,:,:] = img\n",
    "    X_train[n+1,:,:,:] = fliplr(img)\n",
    "    X_train[n+2,:,:,:] = flipud(img)\n",
    "    X_train[n+3,:,:,:] = fliplr(flipud(img))\n",
    "    n += 4\n",
    "    \n",
    "#Populate X_train for multiclass images\n",
    "for file_name in multiclass_samples:\n",
    "    img = imread(IMAGE_PATH + file_name)\n",
    "    #img = color.rgb2gray(io.imread(IMAGE_PATH + file_name))\n",
    "    img = resize(img, (int(IMG_HEIGHT/SCALE_FACTOR), int(IMG_WIDTH/SCALE_FACTOR), IMG_CHANNELS), mode='constant', preserve_range=True)\n",
    "    X_train[n,:,:,:] = img\n",
    "    X_train[n+1,:,:,:] = fliplr(img)\n",
    "    X_train[n+2,:,:,:] = flipud(img)\n",
    "    X_train[n+3,:,:,:] = fliplr(flipud(img))\n",
    "    n += 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Dice Coefficient and a Dice Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_loss(y_true, y_pred):    \n",
    "    return (1-dice_coef(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Models, Initialize with ImageNet Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Ashley\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py:442: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Ashley\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py:112: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Ashley\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py:3543: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Ashley\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py:3386: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Ashley\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py:153: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Ashley\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py:158: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Ashley\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py:1768: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Ashley\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py:3388: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#adamcustom = Adam(lr=0.005)\n",
    "\n",
    "#import tensorflow_addons as tfa\n",
    "#custom_optimizer = tfa.optimizers.AdamW(weight_decay=1e-4)\n",
    "model1 = segmentation_models.Unet('resnet34', encoder_weights='imagenet', input_shape=(int(IMG_HEIGHT/SCALE_FACTOR), int(IMG_WIDTH/SCALE_FACTOR), IMG_CHANNELS), encoder_freeze=True, classes=4, activation='sigmoid')\n",
    "model2 = segmentation_models.Unet('inceptionv3', encoder_weights='imagenet', input_shape=(int(IMG_HEIGHT/SCALE_FACTOR), int(IMG_WIDTH/SCALE_FACTOR), IMG_CHANNELS), encoder_freeze=True, classes=4, activation='sigmoid')\n",
    "model3 = segmentation_models.Unet('densenet121', encoder_weights='imagenet', input_shape=(int(IMG_HEIGHT/SCALE_FACTOR), int(IMG_WIDTH/SCALE_FACTOR), IMG_CHANNELS), encoder_freeze=True, classes=4, activation='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Ashley\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\optimizers.py:711: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Ashley\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py:1188: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "model1.compile('Adam', loss=[dice_loss], metrics=[dice_coef])\n",
    "model2.compile('Adam', loss=[dice_loss], metrics=[dice_coef])\n",
    "model3.compile('Adam', loss=[dice_loss], metrics=[dice_coef])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "XTraining, XValidation, YTraining, YValidation = train_test_split(X_train,Y_train,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(380, 128, 800, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XValidation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3420 samples, validate on 380 samples\n",
      "Epoch 1/25\n",
      "3416/3420 [============================>.] - ETA: 0s - loss: 0.6164 - dice_coef: 0.3836Epoch 00000: val_loss improved from inf to 0.50050, saving model to resnet34_model_shuffled_downsample\n",
      "3420/3420 [==============================] - 415s - loss: 0.6162 - dice_coef: 0.3838 - val_loss: 0.5005 - val_dice_coef: 0.4995\n",
      "Epoch 2/25\n",
      "3416/3420 [============================>.] - ETA: 0s - loss: 0.4970 - dice_coef: 0.5030Epoch 00001: val_loss improved from 0.50050 to 0.45821, saving model to resnet34_model_shuffled_downsample\n",
      "3420/3420 [==============================] - 377s - loss: 0.4971 - dice_coef: 0.5029 - val_loss: 0.4582 - val_dice_coef: 0.5418\n",
      "Epoch 3/25\n",
      "3416/3420 [============================>.] - ETA: 0s - loss: 0.4630 - dice_coef: 0.5370Epoch 00002: val_loss improved from 0.45821 to 0.41257, saving model to resnet34_model_shuffled_downsample\n",
      "3420/3420 [==============================] - 377s - loss: 0.4627 - dice_coef: 0.5373 - val_loss: 0.4126 - val_dice_coef: 0.5874\n",
      "Epoch 4/25\n",
      "3416/3420 [============================>.] - ETA: 0s - loss: 0.4518 - dice_coef: 0.5482Epoch 00003: val_loss improved from 0.41257 to 0.40859, saving model to resnet34_model_shuffled_downsample\n",
      "3420/3420 [==============================] - 376s - loss: 0.4525 - dice_coef: 0.5475 - val_loss: 0.4086 - val_dice_coef: 0.5914\n",
      "Epoch 5/25\n",
      "3416/3420 [============================>.] - ETA: 0s - loss: 0.4250 - dice_coef: 0.5750Epoch 00004: val_loss improved from 0.40859 to 0.40275, saving model to resnet34_model_shuffled_downsample\n",
      "3420/3420 [==============================] - 376s - loss: 0.4249 - dice_coef: 0.5751 - val_loss: 0.4028 - val_dice_coef: 0.5972\n",
      "Epoch 6/25\n",
      "3416/3420 [============================>.] - ETA: 0s - loss: 0.4116 - dice_coef: 0.5884Epoch 00005: val_loss improved from 0.40275 to 0.38467, saving model to resnet34_model_shuffled_downsample\n",
      "3420/3420 [==============================] - 377s - loss: 0.4121 - dice_coef: 0.5879 - val_loss: 0.3847 - val_dice_coef: 0.6153\n",
      "Epoch 7/25\n",
      "3416/3420 [============================>.] - ETA: 0s - loss: 0.4213 - dice_coef: 0.5787Epoch 00006: val_loss did not improve\n",
      "3420/3420 [==============================] - 375s - loss: 0.4211 - dice_coef: 0.5789 - val_loss: 0.3869 - val_dice_coef: 0.6131\n",
      "Epoch 8/25\n",
      "3416/3420 [============================>.] - ETA: 0s - loss: 0.4005 - dice_coef: 0.5995Epoch 00007: val_loss did not improve\n",
      "3420/3420 [==============================] - 375s - loss: 0.4005 - dice_coef: 0.5995 - val_loss: 0.3963 - val_dice_coef: 0.6037\n",
      "Epoch 9/25\n",
      "3416/3420 [============================>.] - ETA: 0s - loss: 0.3869 - dice_coef: 0.6131Epoch 00008: val_loss did not improve\n",
      "3420/3420 [==============================] - 375s - loss: 0.3866 - dice_coef: 0.6134 - val_loss: 0.4082 - val_dice_coef: 0.5918\n",
      "Epoch 10/25\n",
      "3416/3420 [============================>.] - ETA: 0s - loss: 0.3904 - dice_coef: 0.6096Epoch 00009: val_loss improved from 0.38467 to 0.36856, saving model to resnet34_model_shuffled_downsample\n",
      "3420/3420 [==============================] - 376s - loss: 0.3904 - dice_coef: 0.6096 - val_loss: 0.3686 - val_dice_coef: 0.6314\n",
      "Epoch 11/25\n",
      "3416/3420 [============================>.] - ETA: 0s - loss: 0.3806 - dice_coef: 0.6194Epoch 00010: val_loss improved from 0.36856 to 0.35482, saving model to resnet34_model_shuffled_downsample\n",
      "3420/3420 [==============================] - 377s - loss: 0.3806 - dice_coef: 0.6194 - val_loss: 0.3548 - val_dice_coef: 0.6452\n",
      "Epoch 12/25\n",
      "3416/3420 [============================>.] - ETA: 0s - loss: 0.3760 - dice_coef: 0.6240Epoch 00011: val_loss did not improve\n",
      "3420/3420 [==============================] - 376s - loss: 0.3758 - dice_coef: 0.6242 - val_loss: 0.3675 - val_dice_coef: 0.6325\n",
      "Epoch 13/25\n",
      "3416/3420 [============================>.] - ETA: 0s - loss: 0.3755 - dice_coef: 0.6245Epoch 00012: val_loss did not improve\n",
      "3420/3420 [==============================] - 376s - loss: 0.3753 - dice_coef: 0.6247 - val_loss: 0.3593 - val_dice_coef: 0.6407\n",
      "Epoch 14/25\n",
      "3416/3420 [============================>.] - ETA: 0s - loss: 0.3556 - dice_coef: 0.6444Epoch 00013: val_loss improved from 0.35482 to 0.35458, saving model to resnet34_model_shuffled_downsample\n",
      "3420/3420 [==============================] - 377s - loss: 0.3556 - dice_coef: 0.6444 - val_loss: 0.3546 - val_dice_coef: 0.6454\n",
      "Epoch 15/25\n",
      "3416/3420 [============================>.] - ETA: 0s - loss: 0.3595 - dice_coef: 0.6405Epoch 00014: val_loss improved from 0.35458 to 0.35390, saving model to resnet34_model_shuffled_downsample\n",
      "3420/3420 [==============================] - 377s - loss: 0.3593 - dice_coef: 0.6407 - val_loss: 0.3539 - val_dice_coef: 0.6461\n",
      "Epoch 16/25\n",
      "3416/3420 [============================>.] - ETA: 0s - loss: 0.3584 - dice_coef: 0.6416Epoch 00015: val_loss improved from 0.35390 to 0.35292, saving model to resnet34_model_shuffled_downsample\n",
      "3420/3420 [==============================] - 377s - loss: 0.3585 - dice_coef: 0.6415 - val_loss: 0.3529 - val_dice_coef: 0.6471\n",
      "Epoch 17/25\n",
      "3416/3420 [============================>.] - ETA: 0s - loss: 0.3488 - dice_coef: 0.6512Epoch 00016: val_loss did not improve\n",
      "3420/3420 [==============================] - 373s - loss: 0.3490 - dice_coef: 0.6510 - val_loss: 0.3553 - val_dice_coef: 0.6447\n",
      "Epoch 18/25\n",
      "3416/3420 [============================>.] - ETA: 0s - loss: 0.3514 - dice_coef: 0.6486Epoch 00017: val_loss improved from 0.35292 to 0.35189, saving model to resnet34_model_shuffled_downsample\n",
      "3420/3420 [==============================] - 375s - loss: 0.3522 - dice_coef: 0.6478 - val_loss: 0.3519 - val_dice_coef: 0.6481\n",
      "Epoch 19/25\n",
      "3416/3420 [============================>.] - ETA: 0s - loss: 0.3406 - dice_coef: 0.6594Epoch 00018: val_loss did not improve\n",
      "3420/3420 [==============================] - 373s - loss: 0.3405 - dice_coef: 0.6595 - val_loss: 0.3638 - val_dice_coef: 0.6362\n",
      "Epoch 20/25\n",
      "3416/3420 [============================>.] - ETA: 0s - loss: 0.3465 - dice_coef: 0.6535Epoch 00019: val_loss improved from 0.35189 to 0.34048, saving model to resnet34_model_shuffled_downsample\n",
      "3420/3420 [==============================] - 378s - loss: 0.3463 - dice_coef: 0.6537 - val_loss: 0.3405 - val_dice_coef: 0.6595\n",
      "Epoch 21/25\n",
      "3416/3420 [============================>.] - ETA: 0s - loss: 0.3408 - dice_coef: 0.6592Epoch 00020: val_loss did not improve\n",
      "3420/3420 [==============================] - 373s - loss: 0.3408 - dice_coef: 0.6592 - val_loss: 0.3949 - val_dice_coef: 0.6051\n",
      "Epoch 22/25\n",
      "3416/3420 [============================>.] - ETA: 0s - loss: 0.3396 - dice_coef: 0.6604Epoch 00021: val_loss improved from 0.34048 to 0.33981, saving model to resnet34_model_shuffled_downsample\n",
      "3420/3420 [==============================] - 380s - loss: 0.3395 - dice_coef: 0.6605 - val_loss: 0.3398 - val_dice_coef: 0.6602\n",
      "Epoch 23/25\n",
      "3416/3420 [============================>.] - ETA: 0s - loss: 0.3391 - dice_coef: 0.6609Epoch 00022: val_loss improved from 0.33981 to 0.33961, saving model to resnet34_model_shuffled_downsample\n",
      "3420/3420 [==============================] - 377s - loss: 0.3389 - dice_coef: 0.6611 - val_loss: 0.3396 - val_dice_coef: 0.6604\n",
      "Epoch 24/25\n",
      "3416/3420 [============================>.] - ETA: 0s - loss: 0.3296 - dice_coef: 0.6704Epoch 00023: val_loss did not improve\n",
      "3420/3420 [==============================] - 376s - loss: 0.3293 - dice_coef: 0.6707 - val_loss: 0.3527 - val_dice_coef: 0.6473\n",
      "Epoch 25/25\n",
      "3416/3420 [============================>.] - ETA: 0s - loss: 0.3231 - dice_coef: 0.6769Epoch 00024: val_loss improved from 0.33961 to 0.33487, saving model to resnet34_model_shuffled_downsample\n",
      "3420/3420 [==============================] - 381s - loss: 0.3233 - dice_coef: 0.6767 - val_loss: 0.3349 - val_dice_coef: 0.6651\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26c07af30b8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earlystopper = EarlyStopping(patience=7, verbose=1)\n",
    "checkpointer = ModelCheckpoint('resnet34_model_shuffled_downsample', verbose=1, save_best_only=True)\n",
    "model1.fit(XTraining, YTraining, validation_data=(XValidation, YValidation), shuffle=True, batch_size=4, epochs=40,\n",
    "                    callbacks=[earlystopper,checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3800 samples, validate on 380 samples\n",
      "Epoch 1/30\n",
      "3798/3800 [============================>.] - ETA: 0s - loss: 0.6160 - dice_coef: 0.3840Epoch 00000: val_loss improved from inf to 0.47853, saving model to inception_model_shuffled_downsample\n",
      "3800/3800 [==============================] - 794s - loss: 0.6159 - dice_coef: 0.3841 - val_loss: 0.4785 - val_dice_coef: 0.5215\n",
      "Epoch 2/30\n",
      "3798/3800 [============================>.] - ETA: 0s - loss: 0.5220 - dice_coef: 0.4780Epoch 00001: val_loss improved from 0.47853 to 0.46318, saving model to inception_model_shuffled_downsample\n",
      "3800/3800 [==============================] - 654s - loss: 0.5219 - dice_coef: 0.4781 - val_loss: 0.4632 - val_dice_coef: 0.5368\n",
      "Epoch 3/30\n",
      "3798/3800 [============================>.] - ETA: 0s - loss: 0.4983 - dice_coef: 0.5017Epoch 00002: val_loss improved from 0.46318 to 0.42961, saving model to inception_model_shuffled_downsample\n",
      "3800/3800 [==============================] - 653s - loss: 0.4986 - dice_coef: 0.5014 - val_loss: 0.4296 - val_dice_coef: 0.5704\n",
      "Epoch 4/30\n",
      "3798/3800 [============================>.] - ETA: 0s - loss: 0.4737 - dice_coef: 0.5263Epoch 00003: val_loss improved from 0.42961 to 0.40824, saving model to inception_model_shuffled_downsample\n",
      "3800/3800 [==============================] - 654s - loss: 0.4740 - dice_coef: 0.5260 - val_loss: 0.4082 - val_dice_coef: 0.5918\n",
      "Epoch 5/30\n",
      "3798/3800 [============================>.] - ETA: 0s - loss: 0.4558 - dice_coef: 0.5442Epoch 00004: val_loss improved from 0.40824 to 0.40730, saving model to inception_model_shuffled_downsample\n",
      "3800/3800 [==============================] - 654s - loss: 0.4560 - dice_coef: 0.5440 - val_loss: 0.4073 - val_dice_coef: 0.5927\n",
      "Epoch 6/30\n",
      "3798/3800 [============================>.] - ETA: 0s - loss: 0.4535 - dice_coef: 0.5465Epoch 00005: val_loss improved from 0.40730 to 0.38474, saving model to inception_model_shuffled_downsample\n",
      "3800/3800 [==============================] - 655s - loss: 0.4535 - dice_coef: 0.5465 - val_loss: 0.3847 - val_dice_coef: 0.6153\n",
      "Epoch 7/30\n",
      "3798/3800 [============================>.] - ETA: 0s - loss: 0.4349 - dice_coef: 0.5651Epoch 00006: val_loss improved from 0.38474 to 0.38208, saving model to inception_model_shuffled_downsample\n",
      "3800/3800 [==============================] - 653s - loss: 0.4349 - dice_coef: 0.5651 - val_loss: 0.3821 - val_dice_coef: 0.6179\n",
      "Epoch 8/30\n",
      "3798/3800 [============================>.] - ETA: 0s - loss: 0.4342 - dice_coef: 0.5658Epoch 00007: val_loss improved from 0.38208 to 0.36522, saving model to inception_model_shuffled_downsample\n",
      "3800/3800 [==============================] - 655s - loss: 0.4341 - dice_coef: 0.5659 - val_loss: 0.3652 - val_dice_coef: 0.6348\n",
      "Epoch 9/30\n",
      "3798/3800 [============================>.] - ETA: 0s - loss: 0.4243 - dice_coef: 0.5757Epoch 00008: val_loss did not improve\n",
      "3800/3800 [==============================] - 647s - loss: 0.4246 - dice_coef: 0.5754 - val_loss: 0.3658 - val_dice_coef: 0.6342\n",
      "Epoch 10/30\n",
      "3798/3800 [============================>.] - ETA: 0s - loss: 0.4158 - dice_coef: 0.5842Epoch 00009: val_loss improved from 0.36522 to 0.34575, saving model to inception_model_shuffled_downsample\n",
      "3800/3800 [==============================] - 655s - loss: 0.4157 - dice_coef: 0.5843 - val_loss: 0.3457 - val_dice_coef: 0.6543\n",
      "Epoch 11/30\n",
      "3798/3800 [============================>.] - ETA: 0s - loss: 0.4043 - dice_coef: 0.5957Epoch 00010: val_loss did not improve\n",
      "3800/3800 [==============================] - 650s - loss: 0.4043 - dice_coef: 0.5957 - val_loss: 0.3717 - val_dice_coef: 0.6283\n",
      "Epoch 12/30\n",
      "3798/3800 [============================>.] - ETA: 0s - loss: 0.4000 - dice_coef: 0.6000Epoch 00011: val_loss improved from 0.34575 to 0.33925, saving model to inception_model_shuffled_downsample\n",
      "3800/3800 [==============================] - 655s - loss: 0.4000 - dice_coef: 0.6000 - val_loss: 0.3393 - val_dice_coef: 0.6607\n",
      "Epoch 13/30\n",
      "3798/3800 [============================>.] - ETA: 0s - loss: 0.3873 - dice_coef: 0.6127Epoch 00012: val_loss did not improve\n",
      "3800/3800 [==============================] - 646s - loss: 0.3872 - dice_coef: 0.6128 - val_loss: 0.3450 - val_dice_coef: 0.6550\n",
      "Epoch 14/30\n",
      "3798/3800 [============================>.] - ETA: 0s - loss: 0.3855 - dice_coef: 0.6145Epoch 00013: val_loss improved from 0.33925 to 0.32516, saving model to inception_model_shuffled_downsample\n",
      "3800/3800 [==============================] - 650s - loss: 0.3854 - dice_coef: 0.6146 - val_loss: 0.3252 - val_dice_coef: 0.6748\n",
      "Epoch 15/30\n",
      "3798/3800 [============================>.] - ETA: 0s - loss: 0.3820 - dice_coef: 0.6180Epoch 00014: val_loss did not improve\n",
      "3800/3800 [==============================] - 647s - loss: 0.3820 - dice_coef: 0.6180 - val_loss: 0.3275 - val_dice_coef: 0.6725\n",
      "Epoch 16/30\n",
      "3798/3800 [============================>.] - ETA: 0s - loss: 0.3770 - dice_coef: 0.6230Epoch 00015: val_loss improved from 0.32516 to 0.32068, saving model to inception_model_shuffled_downsample\n",
      "3800/3800 [==============================] - 655s - loss: 0.3769 - dice_coef: 0.6231 - val_loss: 0.3207 - val_dice_coef: 0.6793\n",
      "Epoch 17/30\n",
      "3798/3800 [============================>.] - ETA: 0s - loss: 0.3714 - dice_coef: 0.6286Epoch 00016: val_loss did not improve\n",
      "3800/3800 [==============================] - 646s - loss: 0.3713 - dice_coef: 0.6287 - val_loss: 0.3354 - val_dice_coef: 0.6646\n",
      "Epoch 18/30\n",
      "3798/3800 [============================>.] - ETA: 0s - loss: 0.3571 - dice_coef: 0.6429Epoch 00017: val_loss improved from 0.32068 to 0.31850, saving model to inception_model_shuffled_downsample\n",
      "3800/3800 [==============================] - 655s - loss: 0.3571 - dice_coef: 0.6429 - val_loss: 0.3185 - val_dice_coef: 0.6815\n",
      "Epoch 19/30\n",
      "3798/3800 [============================>.] - ETA: 0s - loss: 0.3524 - dice_coef: 0.6476Epoch 00018: val_loss improved from 0.31850 to 0.30194, saving model to inception_model_shuffled_downsample\n",
      "3800/3800 [==============================] - 656s - loss: 0.3523 - dice_coef: 0.6477 - val_loss: 0.3019 - val_dice_coef: 0.6981\n",
      "Epoch 20/30\n",
      "3798/3800 [============================>.] - ETA: 0s - loss: 0.3498 - dice_coef: 0.6502Epoch 00019: val_loss did not improve\n",
      "3800/3800 [==============================] - 649s - loss: 0.3497 - dice_coef: 0.6503 - val_loss: 0.3054 - val_dice_coef: 0.6946\n",
      "Epoch 21/30\n",
      "3798/3800 [============================>.] - ETA: 0s - loss: 0.3542 - dice_coef: 0.6458Epoch 00020: val_loss improved from 0.30194 to 0.29437, saving model to inception_model_shuffled_downsample\n",
      "3800/3800 [==============================] - 660s - loss: 0.3541 - dice_coef: 0.6459 - val_loss: 0.2944 - val_dice_coef: 0.7056\n",
      "Epoch 22/30\n",
      "3798/3800 [============================>.] - ETA: 0s - loss: 0.3458 - dice_coef: 0.6542Epoch 00021: val_loss improved from 0.29437 to 0.29426, saving model to inception_model_shuffled_downsample\n",
      "3800/3800 [==============================] - 654s - loss: 0.3457 - dice_coef: 0.6543 - val_loss: 0.2943 - val_dice_coef: 0.7057\n",
      "Epoch 23/30\n",
      "3798/3800 [============================>.] - ETA: 0s - loss: 0.3395 - dice_coef: 0.6605Epoch 00022: val_loss did not improve\n",
      "3800/3800 [==============================] - 647s - loss: 0.3398 - dice_coef: 0.6602 - val_loss: 0.2966 - val_dice_coef: 0.7034\n",
      "Epoch 24/30\n",
      "3798/3800 [============================>.] - ETA: 0s - loss: 0.3359 - dice_coef: 0.6641Epoch 00023: val_loss improved from 0.29426 to 0.28081, saving model to inception_model_shuffled_downsample\n",
      "3800/3800 [==============================] - 652s - loss: 0.3358 - dice_coef: 0.6642 - val_loss: 0.2808 - val_dice_coef: 0.7192\n",
      "Epoch 25/30\n",
      "3798/3800 [============================>.] - ETA: 0s - loss: 0.3299 - dice_coef: 0.6701Epoch 00024: val_loss did not improve\n",
      "3800/3800 [==============================] - 647s - loss: 0.3299 - dice_coef: 0.6701 - val_loss: 0.2820 - val_dice_coef: 0.7180\n",
      "Epoch 26/30\n",
      "3798/3800 [============================>.] - ETA: 0s - loss: 0.3287 - dice_coef: 0.6713Epoch 00025: val_loss did not improve\n",
      "3800/3800 [==============================] - 647s - loss: 0.3286 - dice_coef: 0.6714 - val_loss: 0.2812 - val_dice_coef: 0.7188\n",
      "Epoch 27/30\n",
      "3798/3800 [============================>.] - ETA: 0s - loss: 0.3269 - dice_coef: 0.6731Epoch 00026: val_loss improved from 0.28081 to 0.27285, saving model to inception_model_shuffled_downsample\n",
      "3800/3800 [==============================] - 655s - loss: 0.3269 - dice_coef: 0.6731 - val_loss: 0.2728 - val_dice_coef: 0.7272\n",
      "Epoch 28/30\n",
      "3798/3800 [============================>.] - ETA: 0s - loss: 0.3208 - dice_coef: 0.6792Epoch 00027: val_loss improved from 0.27285 to 0.26757, saving model to inception_model_shuffled_downsample\n",
      "3800/3800 [==============================] - 656s - loss: 0.3207 - dice_coef: 0.6793 - val_loss: 0.2676 - val_dice_coef: 0.7324\n",
      "Epoch 29/30\n",
      "3798/3800 [============================>.] - ETA: 0s - loss: 0.3117 - dice_coef: 0.6883Epoch 00028: val_loss improved from 0.26757 to 0.26061, saving model to inception_model_shuffled_downsample\n",
      "3800/3800 [==============================] - 655s - loss: 0.3120 - dice_coef: 0.6880 - val_loss: 0.2606 - val_dice_coef: 0.7394\n",
      "Epoch 30/30\n",
      "3798/3800 [============================>.] - ETA: 0s - loss: 0.3140 - dice_coef: 0.6860Epoch 00029: val_loss did not improve\n",
      "3800/3800 [==============================] - 647s - loss: 0.3143 - dice_coef: 0.6857 - val_loss: 0.2617 - val_dice_coef: 0.7383\n"
     ]
    }
   ],
   "source": [
    "earlystopper = EarlyStopping(patience=4, verbose=1)\n",
    "checkpointer = ModelCheckpoint('inception_model_shuffled_downsample', verbose=1, save_best_only=True)\n",
    "results2 = model2.fit(XTraining, YTraining, validation_data=(XValidation, YValidation), shuffle=True, batch_size=3, epochs=30,\n",
    "                    callbacks=[earlystopper,checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3800 samples, validate on 380 samples\n",
      "Epoch 1/50\n",
      "3799/3800 [============================>.] - ETA: 0s - loss: 0.7494 - dice_coef: 0.2506Epoch 00000: val_loss improved from inf to 0.73482, saving model to densenet_model_shuffled_downsample\n",
      "3800/3800 [==============================] - 1581s - loss: 0.7494 - dice_coef: 0.2506 - val_loss: 0.7348 - val_dice_coef: 0.2652\n",
      "Epoch 2/50\n",
      "3799/3800 [============================>.] - ETA: 0s - loss: 0.6916 - dice_coef: 0.3084Epoch 00001: val_loss improved from 0.73482 to 0.71050, saving model to densenet_model_shuffled_downsample\n",
      "3800/3800 [==============================] - 1417s - loss: 0.6915 - dice_coef: 0.3085 - val_loss: 0.7105 - val_dice_coef: 0.2895\n",
      "Epoch 3/50\n",
      "3799/3800 [============================>.] - ETA: 0s - loss: 0.6633 - dice_coef: 0.3367Epoch 00002: val_loss did not improve\n",
      "3800/3800 [==============================] - 1406s - loss: 0.6632 - dice_coef: 0.3368 - val_loss: 0.7734 - val_dice_coef: 0.2266\n",
      "Epoch 4/50\n",
      "3799/3800 [============================>.] - ETA: 0s - loss: 0.6482 - dice_coef: 0.3518Epoch 00003: val_loss did not improve\n",
      "3800/3800 [==============================] - 1408s - loss: 0.6482 - dice_coef: 0.3518 - val_loss: 0.7291 - val_dice_coef: 0.2709\n",
      "Epoch 5/50\n",
      "3799/3800 [============================>.] - ETA: 0s - loss: 0.6404 - dice_coef: 0.3596Epoch 00004: val_loss did not improve\n",
      "3800/3800 [==============================] - 1408s - loss: 0.6405 - dice_coef: 0.3595 - val_loss: 0.7581 - val_dice_coef: 0.2419\n",
      "Epoch 6/50\n",
      "3799/3800 [============================>.] - ETA: 0s - loss: 0.6303 - dice_coef: 0.3697Epoch 00005: val_loss did not improve\n",
      "3800/3800 [==============================] - 1409s - loss: 0.6302 - dice_coef: 0.3698 - val_loss: 0.7391 - val_dice_coef: 0.2609\n",
      "Epoch 7/50\n",
      "3799/3800 [============================>.] - ETA: 0s - loss: 0.6206 - dice_coef: 0.3794Epoch 00006: val_loss did not improve\n",
      "3800/3800 [==============================] - 1410s - loss: 0.6206 - dice_coef: 0.3794 - val_loss: 0.7774 - val_dice_coef: 0.2226\n",
      "Epoch 8/50\n",
      "3799/3800 [============================>.] - ETA: 0s - loss: 0.6131 - dice_coef: 0.3869Epoch 00007: val_loss improved from 0.71050 to 0.69713, saving model to densenet_model_shuffled_downsample\n",
      "3800/3800 [==============================] - 1414s - loss: 0.6132 - dice_coef: 0.3868 - val_loss: 0.6971 - val_dice_coef: 0.3029\n",
      "Epoch 9/50\n",
      "3799/3800 [============================>.] - ETA: 0s - loss: 0.6035 - dice_coef: 0.3965Epoch 00008: val_loss did not improve\n",
      "3800/3800 [==============================] - 1408s - loss: 0.6035 - dice_coef: 0.3965 - val_loss: 0.7567 - val_dice_coef: 0.2433\n",
      "Epoch 10/50\n",
      "3799/3800 [============================>.] - ETA: 0s - loss: 0.6006 - dice_coef: 0.3994Epoch 00009: val_loss did not improve\n",
      "3800/3800 [==============================] - 1410s - loss: 0.6007 - dice_coef: 0.3993 - val_loss: 0.7892 - val_dice_coef: 0.2108\n",
      "Epoch 11/50\n",
      "3799/3800 [============================>.] - ETA: 0s - loss: 0.5928 - dice_coef: 0.4072Epoch 00010: val_loss improved from 0.69713 to 0.69098, saving model to densenet_model_shuffled_downsample\n",
      "3800/3800 [==============================] - 1414s - loss: 0.5927 - dice_coef: 0.4073 - val_loss: 0.6910 - val_dice_coef: 0.3090\n",
      "Epoch 12/50\n",
      "3799/3800 [============================>.] - ETA: 0s - loss: 0.5889 - dice_coef: 0.4111Epoch 00011: val_loss did not improve\n",
      "3800/3800 [==============================] - 1408s - loss: 0.5890 - dice_coef: 0.4110 - val_loss: 0.8197 - val_dice_coef: 0.1803\n",
      "Epoch 13/50\n",
      "3799/3800 [============================>.] - ETA: 0s - loss: 0.5864 - dice_coef: 0.4136Epoch 00012: val_loss did not improve\n",
      "3800/3800 [==============================] - 1410s - loss: 0.5866 - dice_coef: 0.4134 - val_loss: 0.7887 - val_dice_coef: 0.2113\n",
      "Epoch 14/50\n",
      "3799/3800 [============================>.] - ETA: 0s - loss: 0.5771 - dice_coef: 0.4229Epoch 00013: val_loss did not improve\n",
      "3800/3800 [==============================] - 1409s - loss: 0.5772 - dice_coef: 0.4228 - val_loss: 0.7688 - val_dice_coef: 0.2312\n",
      "Epoch 15/50\n",
      "3799/3800 [============================>.] - ETA: 0s - loss: 0.5724 - dice_coef: 0.4276Epoch 00014: val_loss did not improve\n",
      "3800/3800 [==============================] - 1408s - loss: 0.5723 - dice_coef: 0.4277 - val_loss: 0.8178 - val_dice_coef: 0.1822\n",
      "Epoch 16/50\n",
      "3799/3800 [============================>.] - ETA: 0s - loss: 0.5716 - dice_coef: 0.4284Epoch 00015: val_loss did not improve\n",
      "3800/3800 [==============================] - 1408s - loss: 0.5715 - dice_coef: 0.4285 - val_loss: 0.7283 - val_dice_coef: 0.2717\n",
      "Epoch 17/50\n",
      "3799/3800 [============================>.] - ETA: 0s - loss: 0.5656 - dice_coef: 0.4344Epoch 00016: val_loss did not improve\n",
      "3800/3800 [==============================] - 1408s - loss: 0.5657 - dice_coef: 0.4343 - val_loss: 0.8193 - val_dice_coef: 0.1807\n",
      "Epoch 00016: early stopping\n"
     ]
    }
   ],
   "source": [
    "earlystopper = EarlyStopping(patience=5, verbose=1)\n",
    "checkpointer = ModelCheckpoint('densenet_model_shuffled_downsample', verbose=1, save_best_only=True)\n",
    "results3 = model3.fit(X_train, Y_train, validation_data=(XValidation, YValidation), shuffle=True, batch_size=2, epochs=40,\n",
    "                    callbacks=[earlystopper,checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del XTraining\n",
    "del YTraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded test instance 0\n",
      "Loaded test instance 500\n",
      "Loaded test instance 1000\n",
      "Loaded test instance 1500\n",
      "Loaded test instance 2000\n",
      "Loaded test instance 2500\n",
      "Loaded test instance 3000\n",
      "Loaded test instance 3500\n",
      "Loaded test instance 4000\n",
      "Loaded test instance 4500\n",
      "Loaded test instance 5000\n",
      "Loaded test instance 5500\n"
     ]
    }
   ],
   "source": [
    "testfiles = next(os.walk(TEST_IMAGE_PATH))[2]\n",
    "X_test = np.zeros((len(testfiles), int(IMG_HEIGHT/SCALE_FACTOR), int(IMG_WIDTH/SCALE_FACTOR), int(IMG_CHANNELS)), dtype=np.uint8)\n",
    "\n",
    "n = 0\n",
    "for file in testfiles:\n",
    "  \n",
    "    #img = color.rgb2gray(io.imread(TEST_IMAGE_PATH + file))[:,:]\n",
    "    img = imread(TEST_IMAGE_PATH + file)\n",
    "    img = resize(img, (IMG_HEIGHT/SCALE_FACTOR, IMG_WIDTH/SCALE_FACTOR, IMG_CHANNELS), mode='constant', preserve_range=True)\n",
    "    X_test[n,:,:,:] = img\n",
    "    if n % 500 == 0:\n",
    "        print('Loaded test instance', n)\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving entry 0\n",
      "Saving entry 500\n",
      "Saving entry 1000\n",
      "Saving entry 1500\n",
      "Saving entry 2000\n",
      "Saving entry 2500\n",
      "Saving entry 3000\n",
      "Saving entry 3500\n",
      "Saving entry 4000\n",
      "Saving entry 4500\n",
      "Saving entry 5000\n",
      "Saving entry 5500\n"
     ]
    }
   ],
   "source": [
    "submission_list = []\n",
    "model1 = load_model('../input/submission-file/resnet34_model', custom_objects={'dice_loss': dice_loss, 'dice_coef': dice_coef})\n",
    "model2 = load_model(r'C:\\Users\\Ashley\\inception_model_shuffled_downsample', custom_objects={'dice_loss': dice_loss, 'dice_coef': dice_coef})\n",
    "model3 = load_model('../input/submission-file/densenet_model', custom_objects={'dice_loss': dice_loss, 'dice_coef': dice_coef})\n",
    "                   \n",
    "for n in range(len(testfiles)):\n",
    "    #preds_test1 = model1.predict(X_test[n:n+1,:,:,:])\n",
    "    preds_test2 = model2.predict(X_test[n:n+1,:,:,:])\n",
    "    #preds_test3 = model3.predict(X_test[n:n+1,:,:,:])\n",
    "\n",
    "    for m in range(4):\n",
    "        #img1 = preds_test1[0,:,:,m]\n",
    "        img2 = preds_test2[0,:,:,m]\n",
    "        #img3 = preds_test3[0,:,:,m]\n",
    "        #resize mask to original size\n",
    "#         img1 = resize(img, (256, 1600), mode='constant', preserve_range=True)\n",
    "#         img2 = resize(img, (256, 1600), mode='constant', preserve_range=True)\n",
    "#         img3 = resize(img, (256, 1600), mode='constant', preserve_range=True)\n",
    "        #img = np.mean([img1,img2,img3], axis=0)\n",
    "        img2 = resize(img2, (256, 1600), mode='constant', preserve_range=True)\n",
    "        img2 = (img2 > 0.5).astype(np.uint8)\n",
    "        #encode results and put in dataframe\n",
    "        encoded_entry = mask2rle(img2)\n",
    "        row = [testfiles[n] + '_' + str(m+1), encoded_entry]\n",
    "        submission_list.append(row)\n",
    "    if n % 500 == 0:\n",
    "        print('Saving entry', n)\n",
    "    \n",
    "#create submission file\n",
    "submission_data = pd.DataFrame(submission_list, columns=['ImageId_ClassId','EncodedPixels'])\n",
    "submission_data = submission_data.fillna('')\n",
    "submission_data.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
